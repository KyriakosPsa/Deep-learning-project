{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508b8893",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-07T15:07:27.617127Z",
     "iopub.status.busy": "2023-09-07T15:07:27.616635Z",
     "iopub.status.idle": "2023-09-07T15:07:33.053273Z",
     "shell.execute_reply": "2023-09-07T15:07:33.051968Z"
    },
    "papermill": {
     "duration": 5.44617,
     "end_time": "2023-09-07T15:07:33.056185",
     "exception": false,
     "start_time": "2023-09-07T15:07:27.610015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, f1_score\n",
    "from collections import deque  # Import deque for early stopping\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific UserWarning related to y_pred and y_true class mismatch\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"y_pred contains classes not in y_true\")\n",
    "\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "    \"\"\"\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_bal_acc, train_mcc, train_f_score = 0, 0, 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "#         train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "        # get the balanced accuracy\n",
    "        train_bal_acc += balanced_accuracy_score(y.cpu().numpy(), y_pred_class.cpu().numpy())\n",
    "        train_mcc += matthews_corrcoef(y.cpu().numpy(), y_pred_class.cpu().numpy())\n",
    "        train_f_score += f1_score(y.cpu().numpy(), y_pred_class.cpu().numpy(), average='weighted')\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "#     train_acc = train_acc / len(dataloader)\n",
    "    train_bal_acc = train_bal_acc / len(dataloader)\n",
    "    train_mcc = train_mcc / len(dataloader)\n",
    "    train_f_score = train_f_score / len(dataloader)\n",
    "    return train_loss, train_bal_acc, train_mcc, train_f_score\n",
    "\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_bal_acc, test_mcc, test_f_score = 0, 0, 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "#             test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
    "            # get the balanced accuracy\n",
    "            test_bal_acc += balanced_accuracy_score(y.cpu().numpy(), test_pred_labels.cpu().numpy())\n",
    "            test_mcc += matthews_corrcoef(y.cpu().numpy(), test_pred_labels.cpu().numpy())\n",
    "            test_f_score += f1_score(y.cpu().numpy(), test_pred_labels.cpu().numpy(), average='weighted')\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "#     test_acc = test_acc / len(dataloader)\n",
    "    test_bal_acc = test_bal_acc / len(dataloader)\n",
    "    test_mcc = test_mcc / len(dataloader)\n",
    "    test_f_score = test_f_score / len(dataloader)\n",
    "    return test_loss, test_bal_acc, test_mcc, test_f_score\n",
    "\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model: torch.nn.Module,\n",
    "                              train_dataloader: torch.utils.data.DataLoader,\n",
    "                              valid_dataloader: torch.utils.data.DataLoader,\n",
    "                              optimizer: torch.optim.Optimizer,\n",
    "                              loss_fn: torch.nn.Module,\n",
    "                              epochs: int,\n",
    "                              device: torch.device,\n",
    "                              patience: int = 5) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model with early stopping.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    valid_dataloader: A DataLoader instance for the model to be validated on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g., \"cuda\" or \"cpu\").\n",
    "    patience: An integer indicating the number of epochs to wait for improvement\n",
    "              before early stopping (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and validation loss as well as training and\n",
    "    validation metrics. Each metric has a value in a list for each epoch.\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_bal_acc\": [],\n",
    "        \"train_mcc\": [],\n",
    "        \"train_f_score\": [],\n",
    "        \"valid_loss\": [],\n",
    "        \"valid_bal_acc\": [],\n",
    "        \"valid_mcc\": [],\n",
    "        \"valid_f_score\": [],\n",
    "    }\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_model_weights = model.state_dict()\n",
    "\n",
    "    # Create a deque to keep track of the validation loss history\n",
    "    validation_loss_history = deque(maxlen=patience)\n",
    "\n",
    "    # Make sure model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_bal_acc, train_mcc, train_f_score = train_step(model=model,\n",
    "                                                                         dataloader=train_dataloader,\n",
    "                                                                         loss_fn=loss_fn,\n",
    "                                                                         optimizer=optimizer,\n",
    "                                                                         device=device)\n",
    "        valid_loss, valid_bal_acc, valid_mcc, valid_f_score = test_step(model=model,\n",
    "                                                                       dataloader=valid_dataloader,\n",
    "                                                                       loss_fn=loss_fn,\n",
    "                                                                       device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_bal_acc: {train_bal_acc:.4f} | \"\n",
    "            f\"train_mcc: {train_mcc:.4f} | \"\n",
    "            f\"valid_loss: {valid_loss:.4f} | \"\n",
    "            f\"valid_bal_acc: {valid_bal_acc:.4f} | \"\n",
    "            f\"valid_mcc: {valid_mcc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_bal_acc\"].append(train_bal_acc)\n",
    "        results[\"train_mcc\"].append(train_mcc)\n",
    "        results[\"train_f_score\"].append(train_f_score)\n",
    "        results[\"valid_loss\"].append(valid_loss)\n",
    "        results[\"valid_bal_acc\"].append(valid_bal_acc)\n",
    "        results[\"valid_mcc\"].append(valid_mcc)\n",
    "        results[\"valid_f_score\"].append(valid_f_score)\n",
    "\n",
    "        # Append the validation loss to the history\n",
    "        validation_loss_history.append(valid_loss)\n",
    "\n",
    "        # Check if the validation loss improved\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            no_improvement_count = 0\n",
    "            # Save the best model weights\n",
    "            best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # Check if early stopping criteria are met\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # Return the filled results\n",
    "    return results"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.320837,
   "end_time": "2023-09-07T15:07:34.085205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-07T15:07:23.764368",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
