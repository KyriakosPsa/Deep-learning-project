{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Required Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom timeit import default_timer as timer\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom collections import Counter\nimport os\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torchvision import models, transforms, datasets\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom torchinfo import summary\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport pandas as pd\nfrom tempfile import TemporaryDirectory\n\nprint(torch.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T13:52:32.699392Z","iopub.execute_input":"2023-09-12T13:52:32.700383Z","iopub.status.idle":"2023-09-12T13:52:32.709288Z","shell.execute_reply.started":"2023-09-12T13:52:32.700335Z","shell.execute_reply":"2023-09-12T13:52:32.708178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Consider aesthetics \nplt.rcParams['mathtext.fontset'] = 'stix'\nplt.rcParams['font.family'] = 'STIXGeneral'\nplt.rcParams['axes.axisbelow'] = True\nplt.rcParams.update({'font.size': 12})","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:32.736890Z","iopub.execute_input":"2023-09-12T13:52:32.737479Z","iopub.status.idle":"2023-09-12T13:52:32.742838Z","shell.execute_reply.started":"2023-09-12T13:52:32.737442Z","shell.execute_reply":"2023-09-12T13:52:32.741656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import engine\nimport helper_functions","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:32.923700Z","iopub.execute_input":"2023-09-12T13:52:32.924056Z","iopub.status.idle":"2023-09-12T13:52:32.928477Z","shell.execute_reply.started":"2023-09-12T13:52:32.924028Z","shell.execute_reply":"2023-09-12T13:52:32.927361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. **Loading the Dataset**","metadata":{}},{"cell_type":"markdown","source":"As mentioned in the [Data Exploration](https://www.kaggle.com/code/kyriakospsallidas/exploratory-analysis) stage of the project, there is an imbalance in class distribution between the test and validation splits, while the training split is balanced. To address this issue, we merge all the images from the train, test, and validation sub-folders into a unified directory called `fungi-all`.","metadata":{}},{"cell_type":"code","source":"all_directory = '/kaggle/input/fungi-all'\nDATA_DIR = all_directory\ndataset = ImageFolder(DATA_DIR)\nlabels = dataset.targets\nimg_per_class = dict(Counter(dataset.targets))\n\nprint(dataset)\nprint(dataset.class_to_idx)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:32.934250Z","iopub.execute_input":"2023-09-12T13:52:32.934611Z","iopub.status.idle":"2023-09-12T13:52:36.657795Z","shell.execute_reply.started":"2023-09-12T13:52:32.934580Z","shell.execute_reply":"2023-09-12T13:52:36.656826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 2. **Loading the Vision Transformer Base 16 pretrained vision transformer**","metadata":{}},{"cell_type":"markdown","source":"In this notebook we will utilzie the Vision Transformer Base 16 Model, as described in the famous paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929). \nCompared to the golden standard for image classification tasks which are CNNs, transformers offer some novel ideas.\n\nThe key steps of vision transformers are related to the way the input image is handled until the creation of positional embeddings. The remaining architecture is the same as in NLP.\n\n1. The image is split into image patches \n2. Then a CNN with a kernel of the patch size is used to extract features from each patch and create embeddings \n3. Finally self-Attention is calculated between all the image patches ( global context)\n4. The remaining network conforms to the same architecture as in NLP. \n\n\nThe main conceptual difference is the fact that CNNs use moving kernels of fixed size to aggregate local information with a receptieve field that is usually increasing as we go deeper in the netwroks. On the contrary Vision transformers capture global relationships already from the shallow part of the network.\n\n\nRegarding the advantages and disadvantages of Vision Transformers compared to CNNs for Image tasks the main ideas are that:\n1. The Transformer's global focus from the start, makes them demand larger datasets and longer training times than CNNs, which employ a hierarchical approach, progressing from local to global focus.\n\n2. However, when provided with large datasets, Vision Transformers excel in learning intricate patterns and establishing complex relationships.","metadata":{}},{"cell_type":"markdown","source":"<p align=\"center\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:720/0*brmcPLvJpiQWjZpY\" alt=\"ViT\">\n</p>","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nweights =  models.ViT_B_16_Weights.DEFAULT\nvit_model = models.vit_b_16(weights = weights)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:36.660633Z","iopub.execute_input":"2023-09-12T13:52:36.661515Z","iopub.status.idle":"2023-09-12T13:52:37.881778Z","shell.execute_reply.started":"2023-09-12T13:52:36.661480Z","shell.execute_reply":"2023-09-12T13:52:37.880780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. As we can see the vision transformer base 16 initially outputs proabilities for 1.000 classes of the ImageNet dataset it was trained on. \n2. Also as its evident a convolutional layer is used to create the image patches specifically 768 for each image with dimensions of 14 by 14.\n3. Then the 768 patches of each image are flattened into an one dimensional vector of 197 elements wich is fed into the Encoder Decoder layers.","metadata":{}},{"cell_type":"code","source":"summary(vit_model, \n        input_size=(1, 3, 224, 224),\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n       )","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:37.883383Z","iopub.execute_input":"2023-09-12T13:52:37.883948Z","iopub.status.idle":"2023-09-12T13:52:38.022855Z","shell.execute_reply.started":"2023-09-12T13:52:37.883914Z","shell.execute_reply":"2023-09-12T13:52:38.021923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.1 Modify the classification head to fit the 5 class problem**","metadata":{}},{"cell_type":"markdown","source":"As previously mentioned, the initial classification head of the ViT model is designed to handle a classification task of 1,000 different image classes. To adapt the model for our specific problem, which is about only 5 classes, we modify the output shape of the final layer.","metadata":{}},{"cell_type":"code","source":"print(f\"The pre-trained Vision Transformer final classification layer is of the format:\\n{vit_model.heads}\")\n\nnew_heads = nn.Sequential(nn.Linear(in_features=768, out_features=5, bias=True))\nvit_model.heads = new_heads\n\nprint(f\"The updated Vision Transformer final classification layer is of the format:\\n{vit_model.heads}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:38.025691Z","iopub.execute_input":"2023-09-12T13:52:38.026044Z","iopub.status.idle":"2023-09-12T13:52:38.032174Z","shell.execute_reply.started":"2023-09-12T13:52:38.026013Z","shell.execute_reply":"2023-09-12T13:52:38.031169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We go ahead and save the initial weights.","metadata":{}},{"cell_type":"code","source":"init_weights = vit_model.state_dict()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:38.033464Z","iopub.execute_input":"2023-09-12T13:52:38.034236Z","iopub.status.idle":"2023-09-12T13:52:38.044990Z","shell.execute_reply.started":"2023-09-12T13:52:38.034203Z","shell.execute_reply":"2023-09-12T13:52:38.044055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 3. **Definition of appropriate preprocessing transforms**","metadata":{}},{"cell_type":"markdown","source":"As we'll be using PyTorch DataLoaders,we can define specific preprocessing steps that each image batch will pass through before being fed as input into the model. Conveniently, we can adopt the transformations employed in the ViT model. In our scenario, the images are already sized at 224 by 224 pixels, so there's no need for crop or resize operations. The standardization process remains as specified below.","metadata":{}},{"cell_type":"code","source":"# ViT model transforms\ntransform=weights.transforms()\ntransform","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:38.046450Z","iopub.execute_input":"2023-09-12T13:52:38.046864Z","iopub.status.idle":"2023-09-12T13:52:38.056750Z","shell.execute_reply.started":"2023-09-12T13:52:38.046832Z","shell.execute_reply":"2023-09-12T13:52:38.055355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ourtransforms\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:38.057936Z","iopub.execute_input":"2023-09-12T13:52:38.058206Z","iopub.status.idle":"2023-09-12T13:52:38.071153Z","shell.execute_reply.started":"2023-09-12T13:52:38.058183Z","shell.execute_reply":"2023-09-12T13:52:38.070234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4. **Cross Validation**\nTo gain a more comprehensive understanding of our model's performance on data it hasn't seen before than just evaluating its predictions on the single test set, we employ a 5-fold cross-validation approach to derive a mean score.","metadata":{}},{"cell_type":"markdown","source":"### *4.1 Creating cross validation splits*","metadata":{}},{"cell_type":"markdown","source":"First, we begin by splitting our data into training and testing sets. Next, we perform a stratified split specifically on the training data to ensure a balanced representation of each class label in both the training and testing subsets.Finally, we create subsets of the original datasets based on the generated indices while also keeping track of the class label distribution in both sets. Finally, we set up a DataLoader for the test dataset, enabling us to test our model in batches.","metadata":{}},{"cell_type":"code","source":"train_dataset = ImageFolder(DATA_DIR, transform=train_transforms)\ntest_dataset = ImageFolder(DATA_DIR, transform=test_transforms)\n\n# Create a list of indices from 0 to length of dataset\nindices = list(range(len(train_dataset)))\n\n# Retrieve the labels from the dataset\nlabels = [label for _, label in train_dataset]\n\n# Perform stratified split\ntrain_idx, test_idx = train_test_split(\n    indices,\n    test_size=0.15, \n    stratify=labels,\n    random_state=42  # To ensure reproducibility\n)\n\n# Create train, test data with Subset using the indices generated above\ntrain_subset = Subset(train_dataset, train_idx)\ntrain_targets=[train_dataset.targets[i] for i in train_idx]\ntrain_targets_counts=Counter(train_targets)\nprint(train_targets_counts, len(train_subset))\n\ntest_subset = Subset(test_dataset, test_idx)\ntest_targets=[test_dataset.targets[i] for i in test_idx]\ntest_targets_counts=Counter(test_targets)\nprint(test_targets_counts, len(test_subset))\n\n# Create Test DataLoader\ntest_loader = DataLoader(test_subset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:52:38.072693Z","iopub.execute_input":"2023-09-12T13:52:38.073038Z","iopub.status.idle":"2023-09-12T13:53:37.993324Z","shell.execute_reply.started":"2023-09-12T13:52:38.073007Z","shell.execute_reply":"2023-09-12T13:53:37.992365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our next step is to create data loaders for each cross-validation split within the training set. This is accomplished by:\n1. We extract a subset of the dataset for training and validation based on the indices obtained from the stratified split. X contains image paths, and y contains corresponding class labels for the training data.\n\n2. We calculate the counts of each class in both the training and validation sets using the Counter function, and then store these counts in `class_counts_per_split`.\n\n3. We create two subsets from our `reduced_dataset`, one for training and one for validation, based on the indices obtained in this split.\n\n4. We set up data loaders for training and validation.","metadata":{}},{"cell_type":"code","source":"data_loaders = []\nclass_counts_per_split=[]\n\nreduced_dataset = train_subset\n\n\n# Perform cross-validation\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nX = [dataset.imgs[i] for i in train_idx]\ny = [dataset.targets[i] for i in train_idx]\n\nfor i, (train_indices, val_indices) in enumerate(stratified_kfold.split(X, y)):\n    # Get the class labels for the training and testing sets of this split\n    train_classes = [y[idx] for idx in train_indices]\n    val_classes = [y[idx] for idx in val_indices]\n    \n    # Count the occurrences of each class in the training and testing sets\n    train_class_counts = Counter(train_classes)\n    val_class_counts = Counter(val_classes)\n    \n    # Append the counts to the list\n    class_counts_per_split.append((f\"Split {i+1}\", train_class_counts, val_class_counts))\n\n    # Create subsets of the dataset for training and testing using the indices\n    train_subset = Subset(reduced_dataset, train_indices)\n    val_subset = Subset(reduced_dataset, val_indices)\n    \n    # Create data loaders for training and testing\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n    \n    data_loaders.append((train_loader, val_loader))\n\n    # Print class counts for this split\n    print(f\"Split {i+1}:\")\n    print(\"Train Set Class Counts:\", train_class_counts)\n    print(\"Val Set Class Counts:\", val_class_counts)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:53:37.995136Z","iopub.execute_input":"2023-09-12T13:53:37.995502Z","iopub.status.idle":"2023-09-12T13:53:38.022635Z","shell.execute_reply.started":"2023-09-12T13:53:37.995470Z","shell.execute_reply":"2023-09-12T13:53:38.021691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### *4.2 Performing cross validation*","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:53:38.026797Z","iopub.execute_input":"2023-09-12T13:53:38.027618Z","iopub.status.idle":"2023-09-12T13:53:38.034484Z","shell.execute_reply.started":"2023-09-12T13:53:38.027585Z","shell.execute_reply":"2023-09-12T13:53:38.033429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned in the relevant [data exploration notebook](https://www.kaggle.com/code/kyriakospsallidas/exploratory-analysis) the classes in the training set are unbalanced in counts of observations, thus we create weights based on their size compared to the total dataset and we will utilize them in the loss function to create class sensitive learning.","metadata":{}},{"cell_type":"code","source":"total_imgs = len(dataset)\nnum_classes = len(dataset.classes)\nclass_weights = torch.tensor([total_imgs/(class_imgs*num_classes) for class_imgs in img_per_class.values()]).to(device)\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:53:38.035841Z","iopub.execute_input":"2023-09-12T13:53:38.036339Z","iopub.status.idle":"2023-09-12T13:53:38.048499Z","shell.execute_reply.started":"2023-09-12T13:53:38.036278Z","shell.execute_reply":"2023-09-12T13:53:38.047542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We utilize the `engine` script we have created and specifically the `train_with_early_stopping` function which:\n\nTrains and tests a PyTorch model with early stopping based on non-improvment on the validation loss.\n\n```\nArgs:\n- model: A PyTorch model to be trained and tested.\n- train_dataloader: A DataLoader instance for the model to be trained on.\n- valid_dataloader: A DataLoader instance for the model to be validated on.\n- optimizer: A PyTorch optimizer to help minimize the loss function.\n- loss_fn: A PyTorch loss function to calculate loss on both datasets.\n- epochs: An integer indicating how many epochs to train for.\n- device: A target device to compute on (e.g., \"cuda\" or \"cpu\").\n- patience: An integer indicating the number of epochs to wait for improvement\n          before early stopping (default is 5).\n\nReturns:\nA dictionary of training and validation loss as well as training and\nvalidation metrics. Each metric has a value in a list for each epoch.\n```\n\nRegarding the loss function we utilize:\n- `nn.CrossEntropyLoss` whch is a commonly used loss function in classification tasks when dealing with multi-class classification problems.\n\nFor the optimizer we utilize the Adam optimizer with a learning rate of $0.0001$ which is reduced by $0.01$ in each step\n","metadata":{}},{"cell_type":"code","source":"# Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\nresults_all=[]\n# init_weights = vgg16.state_dict()\n\nfor i in range(5):\n    print(f\"Split:{i}\")\n    start_time = timer()\n    \n    # reset the weights in each CV split\n    init_weights =  models.ViT_B_16_Weights.DEFAULT\n    vit_model = models.vit_b_16(weights = init_weights)\n    new_heads = nn.Sequential(nn.Linear(in_features=768, out_features=5, bias=True))\n    vit_model.heads = new_heads\n    print('Model was reset')\n    \n    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = torch.optim.Adam(vit_model.parameters(), lr=0.0001)\n    exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.01)\n    \n    results, _ = engine.train_with_early_stopping(model=vit_model,\n                                                  train_dataloader=data_loaders[i][0],\n                                                  valid_dataloader=data_loaders[i][1],\n                                                  optimizer=optimizer,\n                                                  loss_fn=loss_fn,\n                                                  epochs=20,\n                                                  device=device)\n\n    results_all.append(results)\n\n    # End the timer and print out how long it took\n    end_time = timer()\n    print(f\"[INFO] Total training time of split {i}: {end_time-start_time:.3f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:53:38.050200Z","iopub.execute_input":"2023-09-12T13:53:38.050986Z","iopub.status.idle":"2023-09-12T15:10:46.584930Z","shell.execute_reply.started":"2023-09-12T13:53:38.050914Z","shell.execute_reply":"2023-09-12T15:10:46.583980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we visualize the results:","metadata":{}},{"cell_type":"code","source":"my_dict = {'Balanced Acc': [results_all[i]['valid_bal_acc'][results_all[i]['valid_loss'].index(min(results_all[i]['valid_loss']))] for i in range(len(results_all))], \n           'MCC': [results_all[i]['valid_mcc'][results_all[i]['valid_loss'].index(min(results_all[i]['valid_loss']))] for i in range(len(results_all))], \n           'F1 score': [results_all[i]['valid_f_score'][results_all[i]['valid_loss'].index(min(results_all[i]['valid_loss']))] for i in range(len(results_all))]}\n\nmy_df = pd.DataFrame(my_dict)\nprint(my_df)\nplt.rcParams[\"figure.autolayout\"] = True\nplt.figure(figsize=(10,8))\nsns.set_style('whitegrid')\nax = sns.barplot(data= my_df, estimator=np.mean, color='blue', palette='hls', errorbar=\"sd\", capsize=.2, errwidth=1.5)\nax.bar_label(ax.containers[0], padding=15)\n\nplt.xlabel('Metrics', fontsize=15)\nplt.ylabel(f'Average values', fontsize=15)\n\nplt.suptitle(f'Average score values of 5-fold CV', fontsize=15)\nplt.savefig(f'/kaggle/working/vit_cv_scores.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:10:46.586451Z","iopub.execute_input":"2023-09-12T15:10:46.586882Z","iopub.status.idle":"2023-09-12T15:10:47.291143Z","shell.execute_reply.started":"2023-09-12T15:10:46.586847Z","shell.execute_reply":"2023-09-12T15:10:47.288557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 5. **Final Model Training**  ","metadata":{}},{"cell_type":"markdown","source":"Once we've acquired performance estimates through cross-validation, we can move forward with training the model using a conventional training-validation split and the `engine` script described above. Subsequently, we will save the trained model weights to a specified file path, enabling us to load and make predictions on the test set.","metadata":{}},{"cell_type":"code","source":"def train_vision_transformer(train_dataset, class_weights, device, save_file_name):\n    # Define train and validation indices\n    train_indices, val_indices = train_test_split(\n        list(range(len(train_dataset))),\n        test_size=0.15,\n        stratify=train_dataset.targets,\n        random_state=42\n    )\n    \n    # Create train and validation subsets\n    train_subset = Subset(train_dataset, train_indices)\n    val_subset = Subset(train_dataset, val_indices)\n\n    # Create train and validation loaders\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\n    start_time = timer()\n\n    # Load the model and add a fitting final layer\n    weights = models.ViT_B_16_Weights.DEFAULT\n    vit_model = models.vit_b_16(weights=weights)\n    new_heads = nn.Sequential(nn.Linear(in_features=768, out_features=5, bias=True))\n    vit_model.heads = new_heads\n\n    #  Here we set the loss function & optimizer\n    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = torch.optim.Adam(vit_model.parameters(), lr=0.0001)\n    exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.01)\n\n    print('Start Training of final model')\n    results, final_model = engine.train_with_early_stopping(\n        model=vit_model,\n        train_dataloader=train_loader,\n        valid_dataloader=val_loader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=10,\n        device=device\n    )\n\n    end_time = timer()\n    print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\\n\")\n\n    MODEL_SAVE_PATH = os.path.join('/kaggle/working/',save_file_name)\n    print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n    torch.save(obj=final_model.state_dict(), f=MODEL_SAVE_PATH)\n\n    helper_functions.plot_loss_curves(results)\n    plt.savefig(f'/kaggle/working/{save_file_name}_training_curves.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:10:47.293443Z","iopub.execute_input":"2023-09-12T15:10:47.294149Z","iopub.status.idle":"2023-09-12T15:10:47.306702Z","shell.execute_reply.started":"2023-09-12T15:10:47.294114Z","shell.execute_reply":"2023-09-12T15:10:47.305219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_vision_transformer(train_dataset=train_dataset, class_weights=class_weights, device=device, save_file_name = \"ViT_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:10:47.307993Z","iopub.execute_input":"2023-09-12T15:10:47.308897Z","iopub.status.idle":"2023-09-12T15:28:41.125091Z","shell.execute_reply.started":"2023-09-12T15:10:47.308863Z","shell.execute_reply":"2023-09-12T15:28:41.124195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having trained the model we evaluate it on the held out test set utilizing the `evaluate_model` function:\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, f1_score\n# Initialize the model\nweights = models.ViT_B_16_Weights.DEFAULT\nvit_model = models.vit_b_16(weights=weights)\nnew_heads = nn.Sequential(nn.Linear(in_features=768, out_features=5, bias=True))\nvit_model.heads = new_heads\n\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\n# Load the saved weights\nvit_model.load_state_dict(torch.load(\"ViT_model.pth\"))\nvit_model.to(device)\n\ndef evaluate_model(model, test_loader, loss_fn, device):\n    model.eval()\n    with torch.inference_mode():\n        all_predictions = []\n        all_true_labels = []\n\n        test_loss, test_bal_acc, test_mcc, test_f_score = 0, 0, 0, 0\n        for batch, (X, y) in enumerate(test_loader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n\n            # 1. Forward pass\n            test_pred_logits = model(X)\n            \n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate scores\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_bal_acc += balanced_accuracy_score(y.cpu().numpy(), test_pred_labels.cpu().numpy())\n            test_mcc += matthews_corrcoef(y.cpu().numpy(), test_pred_labels.cpu().numpy())\n            test_f_score += f1_score(y.cpu().numpy(), test_pred_labels.cpu().numpy(), average='weighted')\n\n            all_predictions.extend(test_pred_labels.cpu().numpy())\n            all_true_labels.extend(y.cpu().numpy())\n\n        # Adjust metrics to get average loss and accuracy per batch\n        test_loss = test_loss / len(test_loader)\n        test_bal_acc = test_bal_acc / len(test_loader)\n        test_mcc = test_mcc / len(test_loader)\n        test_f_score = test_f_score / len(test_loader)\n        \n        print(f'Test loss: {test_loss} | Test bal acc {test_bal_acc} | Test mcc {test_mcc} | Test_f_score {test_f_score}\\n')\n        \n        score_dict = {\n            'test_loss': test_loss,\n            'test_bal_acc': test_bal_acc,\n            'test_mcc': test_mcc,\n            'test_f_score': test_f_score,\n            'all_predictions': all_predictions,\n            'all_true_labels': all_true_labels\n        }\n        return score_dict","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:41.126741Z","iopub.execute_input":"2023-09-12T15:28:41.127079Z","iopub.status.idle":"2023-09-12T15:28:42.676839Z","shell.execute_reply.started":"2023-09-12T15:28:41.127048Z","shell.execute_reply":"2023-09-12T15:28:42.675831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = evaluate_model(model = vit_model, test_loader = test_loader, loss_fn = loss_fn , device = device)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:42.678438Z","iopub.execute_input":"2023-09-12T15:28:42.678809Z","iopub.status.idle":"2023-09-12T15:28:50.811096Z","shell.execute_reply.started":"2023-09-12T15:28:42.678775Z","shell.execute_reply":"2023-09-12T15:28:50.810009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = [result[\"test_loss\"],result[\"test_bal_acc\"],result[\"test_mcc\"]]\nmetrics = [\"Loss\",\"Balanced Accuracy\",\"MCC\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:50.812478Z","iopub.execute_input":"2023-09-12T15:28:50.813111Z","iopub.status.idle":"2023-09-12T15:28:50.818913Z","shell.execute_reply.started":"2023-09-12T15:28:50.813076Z","shell.execute_reply":"2023-09-12T15:28:50.817881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1 *Final Model performance on the test set*","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nsns.barplot(x=scores,y = metrics, palette=\"viridis\")\nplt.xticks(np.arange(0.0,1.1,0.1))\nplt.xlabel('Score')\nplt.ylabel('Metric')\nplt.title('Classification Metrics')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:50.822860Z","iopub.execute_input":"2023-09-12T15:28:50.823196Z","iopub.status.idle":"2023-09-12T15:28:51.324212Z","shell.execute_reply.started":"2023-09-12T15:28:50.823166Z","shell.execute_reply":"2023-09-12T15:28:51.323286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(result['all_true_labels'], result['all_predictions'])\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig(f'/kaggle/working/ViT_final_model_cm.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:51.325701Z","iopub.execute_input":"2023-09-12T15:28:51.326050Z","iopub.status.idle":"2023-09-12T15:28:52.491159Z","shell.execute_reply.started":"2023-09-12T15:28:51.326016Z","shell.execute_reply":"2023-09-12T15:28:52.490225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 6. **Final Model training with Data Augmentation to reduce overfitting**","metadata":{}},{"cell_type":"markdown","source":"As observed above the final model is overfitting on the training set. To address this issue we employ data augmentation. By introducing variability into the dataset, data augmentation helps prevent the model from excessively fitting to the specific noise present in the training data.\n\nWe utilize:\n1. A Horizontal flip either left or right \n2. A random rotation of 10 degrees \n3. A random shift in brigtness, contrast, saturation and hue","metadata":{}},{"cell_type":"code","source":"# Define train and val transformations\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:52.492525Z","iopub.execute_input":"2023-09-12T15:28:52.493107Z","iopub.status.idle":"2023-09-12T15:28:52.503313Z","shell.execute_reply.started":"2023-09-12T15:28:52.493073Z","shell.execute_reply":"2023-09-12T15:28:52.502368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload training data with augmentations\ntrain_indices, val_indices = train_test_split(train_idx, test_size=0.15, stratify=train_targets, random_state=42)\n\ntrain_dataset = ImageFolder(DATA_DIR, transform=train_transforms)\nval_dataset = ImageFolder(DATA_DIR, transform=val_transforms)\n\ntrain_subset = Subset(train_dataset, train_indices)\ntrain_targets=[train_dataset.targets[i] for i in train_indices]\n                                              \nval_subset = Subset(val_dataset, val_indices)\nval_targets=[train_dataset.targets[i] for i in val_indices]\n                                              \ntrain_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n                                              \n                                              \ntrain_vision_transformer(train_dataset=train_dataset, class_weights=class_weights, device=device, save_file_name = \"ViT_model_aug.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:28:52.504921Z","iopub.execute_input":"2023-09-12T15:28:52.505195Z","iopub.status.idle":"2023-09-12T15:57:38.128711Z","shell.execute_reply.started":"2023-09-12T15:28:52.505171Z","shell.execute_reply":"2023-09-12T15:57:38.127609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1 *Post data augmentation Final Model performance on the test set*","metadata":{}},{"cell_type":"code","source":"# Load the saved weights\nweights = models.ViT_B_16_Weights.DEFAULT\nvit_model_aug = models.vit_b_16(weights=weights)\nnew_heads = nn.Sequential(nn.Linear(in_features=768, out_features=5, bias=True))\nvit_model_aug.heads = new_heads\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\nvit_model_aug.load_state_dict(torch.load(\"ViT_model_aug.pth\"))\nvit_model_aug.to(device)\n\n# Evaluate on the test set\nresult = evaluate_model(model = vit_model_aug, test_loader = test_loader, loss_fn = loss_fn , device = device)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:57:38.130415Z","iopub.execute_input":"2023-09-12T15:57:38.130789Z","iopub.status.idle":"2023-09-12T15:57:47.808781Z","shell.execute_reply.started":"2023-09-12T15:57:38.130755Z","shell.execute_reply":"2023-09-12T15:57:47.807839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(result['all_true_labels'], result['all_predictions'])\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.savefig(f'/kaggle/working/ViT_final_model_cm.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:57:47.812793Z","iopub.execute_input":"2023-09-12T15:57:47.815492Z","iopub.status.idle":"2023-09-12T15:57:49.111260Z","shell.execute_reply.started":"2023-09-12T15:57:47.815454Z","shell.execute_reply":"2023-09-12T15:57:49.109424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--- ","metadata":{}},{"cell_type":"markdown","source":"## 7. **Visualizing the forward pass of the Vision Transformer & Attention**","metadata":{}},{"cell_type":"markdown","source":"In order to have a better intuition on how the Vision Transformer Base 16 model function, we can attempt to visualize the intermediate outputs of the main layers as an image is passed into the forward pass loop of the trained model on the augmented dataset which produced the best results. ","metadata":{}},{"cell_type":"code","source":"def image_pass(model, image, y_true):\n    image_batch = image.unsqueeze(0).to(device)\n    model.to(device)\n    model.eval()\n\n    # Define a list to store the hook's output\n    att_hook_output = []\n    conv_hook_output = []\n    \n    # Define the hook callback function\n    def conv_layer_hook(module,input, output):\n        conv_hook_output.append(output[0])\n    \n    conv_layers_count = 0\n    for name, module in vit_model_aug.named_modules():\n        if isinstance(module, nn.Conv2d):\n            conv_hook = module.register_forward_hook(conv_layer_hook)\n            conv_layers_count += 1\n    print(f\"There are a total of {conv_layers_count} Convolutional layers\\n\")\n    \n    def multi_head_attention_hook(module, input, output):\n        att_hook_output.append(output[0])\n\n    # Register the hook on the specific layer\n    attention_layers_count = 0\n    for name, module in vit_model_aug.named_modules():\n        if isinstance(module, nn.MultiheadAttention):\n            att_hook = module.register_forward_hook(multi_head_attention_hook)\n            attention_layers_count += 1\n    print(f\"There are a total of {attention_layers_count} Multi head attention layers\\n\")\n    \n    # Perform the forward pass\n    output = model(image_batch)\n    print(\"All forward pass hooks registered \\n\")\n    \n    return output, att_hook_output, conv_hook_output","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:57:49.117622Z","iopub.execute_input":"2023-09-12T15:57:49.118009Z","iopub.status.idle":"2023-09-12T15:57:49.137936Z","shell.execute_reply.started":"2023-09-12T15:57:49.117974Z","shell.execute_reply":"2023-09-12T15:57:49.136751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def visualize_process(image,conv_hook_results,att_hook_results,y_true,output,img_id):\n    # 1. Plot the input image\n    print(\"---------- 1. The original input image is: ----------\\n\")\n    plt.figure()\n    image = image.squeeze().cpu().detach().numpy()\n    image = image.transpose((1, 2, 0))\n    plt.title(f\"Input Image, true class: {y_true}\")\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.savefig(f\"input_img_{id}.jpg\",dpi = 300)\n    plt.show()\n    \n    # 2. Plot the patches produced from the Convolutional layer\n    conv_results = conv_hook_results[0].squeeze().cpu().detach().numpy()\n    print(\"---------- 2. Example image patches from the convolutional layer: ----------\\n\")\n    fig, axes = plt.subplots( 4, 5, figsize=(12, 10))\n    plt.suptitle(\"Image patches\")\n    for i, ax in enumerate(axes.ravel()):\n        if i < 20:\n            ax.imshow(conv_results[i,:,:], cmap = \"gray\")\n            ax.set_title(f\"Patch {i + 1} out of 768\")\n            ax.axis('off')  # Hide axis labels\n    plt.tight_layout()\n    plt.savefig(f\"cnn_patch_img_{id}.jpg\",dpi = 300)\n    plt.show()\n    \n    # Flattened patch\n    print(\"---------- 3. Flattened image patches from the convolutional layer: ----------\\n\")\n    fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n    plt.suptitle(\"Flattened Image patches:\")\n    for i, ax in enumerate(axes.ravel()):\n        if i < 20:\n            ax.hist(conv_results[i,:,:].flatten(),bins=50)\n            ax.set_title(f\"Patch {i + 1} out of 768\")\n            ax.set_xlabel('Counts')\n            ax.set_ylabel('Values')\n    plt.tight_layout()\n    plt.savefig(f\"cnn_flatpatch_img_{id}.jpg\",dpi = 300)\n    plt.show()\n    \n    # 3. Plot the attention weights between the pixels of the patches\n    print(\"----------  4. Attention layer output for all 768 image patches and each of their 196 pixels: ------------\\n\")\n    fig,axes = plt.subplots(3,4, figsize=(22, 20))\n    plt.suptitle(\"Image Patch to Image patch, Attention output visualization: 768 image patches of embedding shape 1 by 196\")\n    for i, ax in enumerate(axes.ravel()):\n        att_results = att_hook_results[i].squeeze().cpu().detach().numpy()\n        if i < 20:\n            sns.heatmap(att_results.T, cmap=\"gray\", ax=ax, robust = True)\n            ax.set_xlabel('Vector Embedding Index')\n            ax.set_ylabel('Image Patch Index')\n            ax.set_title(f\"Weights for attention layer {i + 1} out of 12\")\n    plt.tight_layout()\n    plt.savefig(f\"att_img_{id}.jpg\",dpi = 300)\n    plt.show()\n    \n    # 4. Plot pred proababilites of the models outcome\n    print(\"---------- 5. Output of Softmax on the model predicted logits ------------\\n\")\n    output = output[0].cpu().detach().numpy()\n    # Use softmax \n    exp_x = np.exp(output)\n    sum_exp_x = np.sum(exp_x, axis=0)\n    softmax_probs = exp_x / sum_exp_x\n    plt.figure()\n    plt.title(f\"Model prediction: {np.argmax(softmax_probs)} | True class: {y_true}\")\n    sns.barplot(x = [\"Class 0\",\"Class 1\",\"Class 2\",\"Class 3\", \"Class 4\"], y = softmax_probs)\n    plt.savefig(f\"pred_img_{id}.jpg\",dpi = 300)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:57:49.139694Z","iopub.execute_input":"2023-09-12T15:57:49.140132Z","iopub.status.idle":"2023-09-12T15:57:49.160354Z","shell.execute_reply.started":"2023-09-12T15:57:49.140099Z","shell.execute_reply":"2023-09-12T15:57:49.159430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_id = 600\nmodel = vit_model_aug\nimage = test_dataset[img_id][0]\ny_true =  test_dataset[img_id][1]\n\noutput, att_hook_output, conv_hook_output = image_pass(model, image, y_true)\nvisualize_process(image,conv_hook_output, att_hook_output, y_true, output, img_id)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T15:57:49.161760Z","iopub.execute_input":"2023-09-12T15:57:49.162230Z","iopub.status.idle":"2023-09-12T15:59:02.581073Z","shell.execute_reply.started":"2023-09-12T15:57:49.162196Z","shell.execute_reply":"2023-09-12T15:59:02.580167Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}