{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Required Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom collections import Counter\nimport os\nfrom torchvision import models, transforms, datasets\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom torchinfo import summary\nimport time\nimport torch\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\n\nfrom tempfile import TemporaryDirectory\n\n# cudnn.benchmark = True\n# plt.ion() \nprint(torch.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-08T11:15:17.843548Z","iopub.execute_input":"2023-09-08T11:15:17.843937Z","iopub.status.idle":"2023-09-08T11:15:17.853176Z","shell.execute_reply.started":"2023-09-08T11:15:17.843906Z","shell.execute_reply":"2023-09-08T11:15:17.852195Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"2.0.0+cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. **Loading the Dataset**","metadata":{}},{"cell_type":"markdown","source":"As mentioned in the Data Exploration notebook, there is an imbalance in class distribution between the test and validation splits, while the training split is balanced. To address this issue, we merge all the images from the train, test, and validation sub-folders into a unified directory called `fungi-all`.","metadata":{}},{"cell_type":"code","source":"directory_to_check = '/kaggle/input/fungi-all'\n\n# Check if the directory exists\nif not os.path.exists(directory_to_check):\n    # Define the source and destination directories\n    source_base_dir = '/kaggle/input/microscopic-fungi-images'\n    destination_dir = '/kaggle/working/fungi-all'\n\n    # List of subdirectories to merge\n    subdirectories_to_merge = ['H1', 'H2', 'H3', 'H5', 'H6']\n\n    # Create the destination directory if it doesn't exist\n    if not os.path.exists(destination_dir):\n        os.makedirs(destination_dir)\n\n    # Loop through each of the train, test, and val directories\n    for dataset_dir in ['train', 'test', 'valid']:\n        # Loop through the subdirectories to merge\n        for subdirectory in subdirectories_to_merge:\n            # Define the source and destination paths\n            source_path = os.path.join(source_base_dir, dataset_dir, subdirectory)\n            destination_path = os.path.join(destination_dir, subdirectory)\n\n            # Create a directory for the class if it doesn't exist\n            if not os.path.exists(destination_path):\n                os.makedirs(destination_path)\n\n            # Copy the files from source to destination\n            for filename in os.listdir(source_path):\n                source_file = os.path.join(source_path, filename)\n                destination_file = os.path.join(destination_path, filename)\n                shutil.copy(source_file, destination_file)\n\n        # Define the directory path\n        all_directory = '/kaggle/working/fungi-all'\n\n    print(\"Files merged successfully.\")\n            \nelse:\n        all_directory = '/kaggle/input/fungi-all'\n        print(\"fungi-all exists\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:15:17.861097Z","iopub.execute_input":"2023-09-08T11:15:17.861725Z","iopub.status.idle":"2023-09-08T11:15:17.874437Z","shell.execute_reply.started":"2023-09-08T11:15:17.861688Z","shell.execute_reply":"2023-09-08T11:15:17.873472Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"fungi-all exists\n","output_type":"stream"}]},{"cell_type":"code","source":"DATA_DIR = all_directory\n\ndataset = ImageFolder(DATA_DIR)\nlabels = dataset.targets\nimg_per_class = dict(Counter(dataset.targets))\n\nprint(dataset)\nprint(dataset.class_to_idx)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:15:17.876264Z","iopub.execute_input":"2023-09-08T11:15:17.877244Z","iopub.status.idle":"2023-09-08T11:15:19.197076Z","shell.execute_reply.started":"2023-09-08T11:15:17.877208Z","shell.execute_reply":"2023-09-08T11:15:19.195932Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Dataset ImageFolder\n    Number of datapoints: 6801\n    Root location: /kaggle/input/fungi-all\n{'H1': 0, 'H2': 1, 'H3': 2, 'H5': 3, 'H6': 4}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 2. **Loading the Vision Transformer Base 16 pretrained vision transformer**","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nweights =  models.ViT_B_16_Weights.DEFAULT\nvit_model = models.vit_b_16(weights = weights)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:15:19.210528Z","iopub.execute_input":"2023-09-08T11:15:19.211270Z","iopub.status.idle":"2023-09-08T11:15:20.895268Z","shell.execute_reply.started":"2023-09-08T11:15:19.211228Z","shell.execute_reply":"2023-09-08T11:15:20.894253Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"The novelty of vision ","metadata":{}},{"cell_type":"markdown","source":"<p align=\"center\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:720/0*brmcPLvJpiQWjZpY\" alt=\"ViT\">\n</p>","metadata":{}},{"cell_type":"code","source":"summary(vit_model, \n        input_size=(1, 3, 224, 224),\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n       )","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:41:52.488110Z","iopub.execute_input":"2023-09-08T11:41:52.488586Z","iopub.status.idle":"2023-09-08T11:41:52.932499Z","shell.execute_reply.started":"2023-09-08T11:41:52.488548Z","shell.execute_reply":"2023-09-08T11:41:52.931253Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nVisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 1000]            768                  True\n├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              True\n│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   True\n│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        1,536                True\n├─Sequential (heads)                                         [1, 768]             [1, 1000]            --                   True\n│    └─Linear (head)                                         [1, 768]             [1, 1000]            769,000              True\n============================================================================================================================================\nTotal params: 86,567,656\nTrainable params: 86,567,656\nNon-trainable params: 0\nTotal mult-adds (M): 173.23\n============================================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 104.09\nParams size (MB): 232.27\nEstimated Total Size (MB): 336.96\n============================================================================================================================================"},"metadata":{}}]},{"cell_type":"code","source":"vit_model\ntransform=weights.transforms()\ntransform","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:15:20.900438Z","iopub.execute_input":"2023-09-08T11:15:20.900848Z","iopub.status.idle":"2023-09-08T11:15:20.908603Z","shell.execute_reply.started":"2023-09-08T11:15:20.900813Z","shell.execute_reply":"2023-09-08T11:15:20.907380Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)"},"metadata":{}}]},{"cell_type":"code","source":"total_imgs = len(dataset)\nnum_classes = len(dataset.classes)\nweights = torch.tensor([total_imgs/(class_imgs*num_classes) for class_imgs in img_per_class.values()]).to(device)\nprint(weights)\n\n# define the CrossEntropyLoss with weights\nloss_fn = nn.CrossEntropyLoss(weight=weights)\noptimizer = torch.optim.Adam(vit_model.parameters(), lr=0.0001)\nexp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:15:31.377727Z","iopub.execute_input":"2023-09-08T11:15:31.378180Z","iopub.status.idle":"2023-09-08T11:15:31.389150Z","shell.execute_reply.started":"2023-09-08T11:15:31.378141Z","shell.execute_reply":"2023-09-08T11:15:31.387637Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"tensor([0.7258, 0.9285, 1.1696, 1.1726, 1.1942])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:15:19.199143Z","iopub.execute_input":"2023-09-08T11:15:19.199939Z","iopub.status.idle":"2023-09-08T11:15:19.208447Z","shell.execute_reply.started":"2023-09-08T11:15:19.199894Z","shell.execute_reply":"2023-09-08T11:15:19.207133Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}]},{"cell_type":"code","source":"vit_model.classifier","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:24:51.501989Z","iopub.execute_input":"2023-09-08T11:24:51.502379Z","iopub.status.idle":"2023-09-08T11:24:51.567286Z","shell.execute_reply.started":"2023-09-08T11:24:51.502348Z","shell.execute_reply":"2023-09-08T11:24:51.565768Z"},"trusted":true},"execution_count":55,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'classifier'"],"ename":"AttributeError","evalue":"'VisionTransformer' object has no attribute 'classifier'","output_type":"error"}]}]}